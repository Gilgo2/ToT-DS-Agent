{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "import langchain_core\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AutoKaggle Project\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"codegemma\")\n",
    "\n",
    "#llm.invoke(\"Write me a Python function to calculate the nth fibonacci number. only include the code provide no explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.read_csv('./data/titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "prompt_path = './prompts/flow_prompts'\n",
    "prompts = []\n",
    "for filename in os.listdir(prompt_path):\n",
    "    with open(os.path.join(prompt_path, filename)) as prompt_f:\n",
    "        prompt = prompt_f.read()\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFSMemory:\n",
    "    pass\n",
    "    #https://github.com/langchain-ai/langchain/blob/master/libs/experimental/langchain_experimental/tot/memory.py\n",
    "    # Memory can be dynamic, we don't need the whole conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_regex = {'codegemma':\"`python\\n([^`]+?)`\"}\n",
    "class AutoKaggle:\n",
    "    def __init__(self, llm, dataset):\n",
    "        #todo: there should be an output variable in the template\n",
    "        self.init_template = init_template\n",
    "        self.llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=ConversationBufferMemory(memory_key=\"result_history\"))\n",
    "        self.llm_parse_regex = parsing_regex[llm]\n",
    "        #todo: load dataset\n",
    "    \n",
    "    def run_step(self, task_name, prompt, output_variable, should_share_results_with_model=False):\n",
    "        completed = False\n",
    "        while not completed:\n",
    "            res = llm_chain.predict(task_prompt=f'{task_name}:\\n{prompt}')\n",
    "            \n",
    "            try:\n",
    "                if len(code_blocks) == 0:\n",
    "                    raise\n",
    "                code_blocks = re.findall(self.llm_parse_regex, res, flags=re.DOTALL)\n",
    "                for code in code_blocks:\n",
    "                    exec(code)\n",
    "            except Exception as e:\n",
    "                #Seems like telling the model to regenerate the code with the last error only makes it repeat the error\n",
    "                res = llm_chain.predict(task_prompt=f\"I received the error for your last code block {code}: {e}. Can you regenerate the code?\")\n",
    "                code_blocks = re.findall(self.llm_parse_regex, res, flags=re.DOTALL)\n",
    "            else:\n",
    "                completed = True\n",
    "            if should_share_results_with_model:\n",
    "                memory.chat_memory.messages.append(langchain_core.messages.human.HumanMessage(f'{task_name} results: {output_variable[:200]}' ))\n",
    "\n",
    "    def llm_exploration(self):\n",
    "        exploration_prompt = \"\"\n",
    "        run('exploration', exploration_prompt, 'exploration_string')\n",
    "\n",
    "    def llm_preprocessing(self):\n",
    "        pass\n",
    "    def llm_model_selection(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        # Implement tree logic with DFSMemory\n",
    "        pass \n",
    "    \n",
    "        \n",
    "    def evaluate(self):\n",
    "        # Evaluate the best_model results on the test data\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: template needs to get output_variable option and then we can assert on it\n",
    "init_template = f\"\"\"Hello, today your job is to be a data scientist assistant for classic ML problems.\n",
    "You will be given a dataset with full explanation about the columns and about the task\n",
    "your job will be to provide all the code to solve the challenge.\n",
    "Since this is a classical ML challenge we are looking to do all the following steps:\n",
    "Data Exploration and Understanding,Data Preprocessing,Model Selection,Model Training,Model Refinement\n",
    "Try to be creative as this will be a trial and error based approach.\n",
    "\n",
    "dataset info:\n",
    "{prompts[1]}\n",
    "\n",
    "New Task:\n",
    "{{task_prompt}}\n",
    "\"\"\"\n",
    "#memory = ConversationBufferMemory(memory_key=\"result_history\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"result_history\", \"dataset_descritpion\", \"task_prompt\"], template=init_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "   # memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "completed = False\n",
    "res = llm_chain.predict(task_prompt='exploration:\\n' + prompts[2])\n",
    "code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "while not completed:\n",
    "    try:\n",
    "        if len(code_blocks) == 0:\n",
    "            raise\n",
    "        for code in code_blocks:\n",
    "            print(f\"----Executing {code}\")\n",
    "            exec(code)\n",
    "    except Exception as e:\n",
    "        print(\"error\", e)\n",
    "        #Seems like telling the model to regenerate the code with the last error only makes it repeat the error\n",
    "        #res = llm_chain.predict(task_prompt=f\"I received the error for your last code block. Can you regenerate the code?\")\n",
    "        res = llm_chain.predict(task_prompt='exploration:\\n' + prompts[2])\n",
    "        code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "    else:\n",
    "        completed = True\n",
    "memory.chat_memory.messages.append(langchain_core.messages.human.HumanMessage('exploration results:' + exploration_string[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = False\n",
    "res = llm_chain.predict(task_prompt=f'Given this exploration string: {exploration_string} Now do data processing: {prompts[4]}\\n' )\n",
    "code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "while not completed:\n",
    "    try:\n",
    "        if len(code_blocks) == 0:\n",
    "            raise\n",
    "        for code in code_blocks:\n",
    "            print(f\"----Executing {code}\")\n",
    "            exec(code)\n",
    "    except Exception as e:\n",
    "        print(\"error\", e)\n",
    "        #Seems like telling the model to regenerate the code with the last error only makes it repeat the error\n",
    "        #res = llm_chain.predict(task_prompt=f\"I received the error for your last code block {code_blocks}: {e}. Can you regenerate the code?\")\n",
    "        #todo implement a check to see all variables are not strings\n",
    "        res = llm_chain.predict(task_prompt=f'Given this exploration string: {exploration_string} Now do data processing: {prompts[4]}\\n' )\n",
    "        code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "    else:\n",
    "        completed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocess_data(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_data.drop(columns=['Survived'])\n",
    "y = preprocessed_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data.drop(columns=['Survived']), preprocessed_data['Survived'],\n",
    "                                                    test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = False\n",
    "res = llm_chain.predict(task_prompt='Model Selection:\\n' + prompts[6])\n",
    "print(res)\n",
    "code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "while not completed:\n",
    "    try:\n",
    "        if len(code_blocks) == 0:\n",
    "            raise\n",
    "        for code in code_blocks:\n",
    "            print(f\"----Executing {code}\")\n",
    "            exec(code)\n",
    "    except Exception as e:\n",
    "        res = llm_chain.predict(task_prompt=f\"I received the error for your last code block {code}: {e}. Can you regenerate the code?\")\n",
    "        code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "    else:\n",
    "        completed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = False\n",
    "res = llm_chain.predict(task_prompt='Model Training:\\n' + prompts[5])\n",
    "code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "while not completed:\n",
    "    try:\n",
    "        if len(code_blocks) == 0:\n",
    "            raise\n",
    "        for code in code_blocks:\n",
    "            print(f\"----Executing {code}\")\n",
    "            exec(code)\n",
    "    except Exception as e:\n",
    "        res = llm_chain.predict(task_prompt=f\"I received the error for your last code block {code}: {e}. Can you regenerate the code?\")\n",
    "        code_blocks = re.findall(\"`python\\n([^`]+?)`\", res, flags=re.DOTALL)\n",
    "    else:\n",
    "        completed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
